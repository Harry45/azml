{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ddf55db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.nn import Dropout\n",
    "from torch.optim import Optimizer\n",
    "import torch.nn.functional as F\n",
    "from transformers import XLMTokenizer\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict, Callable, Optional\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46200712",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "PAD = 0\n",
    "UNK = 1\n",
    "MAX_VOCAB_SIZE = 50000\n",
    "SPECIAL_TOKENS_OFFSET = 2\n",
    "NTRAIN = None\n",
    "MAX_LENGTH = 100\n",
    "BATCH_SIZE = 128\n",
    "NEPOCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34585639",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =pd.read_csv(\"data/translation/en2fr.csv\", index_col=0)\n",
    "\n",
    "if NTRAIN is None:\n",
    "    data = data.reset_index(drop=True)\n",
    "else:\n",
    "    data = data.reset_index(drop=True).iloc[0:NTRAIN]\n",
    "\n",
    "tokenizer = XLMTokenizer.from_pretrained(\"xlm-clm-enfr-1024\")\n",
    "num_examples=len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be03346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples available is 47173.\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of examples available is {num_examples}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55d63acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionary(data: pd.DataFrame, tokenizer, language: str = 'en') -> Tuple[Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"\n",
    "    Creates a word-to-index and index-to-word dictionary from tokenized sentences in a specified language.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): A pandas DataFrame containing text data. The column with sentences should be named according to the language.\n",
    "        tokenizer: A tokenizer object with a `tokenize` method to tokenize each sentence.\n",
    "        language (str): The language code for the column in the DataFrame to be tokenized (default is 'en' for English).\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, int], Dict[int, str]]: \n",
    "        - word_dict: A dictionary mapping each word to its unique index.\n",
    "        - idx_dict: A dictionary mapping indices back to words.\n",
    "    \"\"\"\n",
    "    # Extract sentences from the specified language column\n",
    "    sentences = data[language]\n",
    "    \n",
    "    # Tokenize sentences, adding 'BOS' (Beginning of Sentence) and 'EOS' (End of Sentence) tokens\n",
    "    tokens = [[\"BOS\"] + tokenizer.tokenize(sentence) + [\"EOS\"] for sentence in sentences]\n",
    "    \n",
    "    # Count the frequency of each word in the tokenized sentences\n",
    "    word_count = Counter(word for sentence in tokens for word in sentence)\n",
    "    \n",
    "    # Get the most common words up to the maximum vocabulary size\n",
    "    most_common_words = word_count.most_common(MAX_VOCAB_SIZE)\n",
    "    \n",
    "    # Create word-to-index dictionary, starting from the offset to reserve space for special tokens\n",
    "    word_dict = {word: idx + SPECIAL_TOKENS_OFFSET for idx, (word, _) in enumerate(most_common_words)}\n",
    "    \n",
    "    # Add special tokens to the dictionary\n",
    "    word_dict[\"PAD\"] = PAD\n",
    "    word_dict[\"UNK\"] = UNK\n",
    "    \n",
    "    # Create index-to-word dictionary (inverse of word_dict)\n",
    "    idx_dict = {idx: word for word, idx in word_dict.items()}\n",
    "    \n",
    "    return tokens, word_dict, idx_dict\n",
    "\n",
    "\n",
    "def idx_to_sentence(ids: List[int], idx_dict: Dict[int, str]) -> str:\n",
    "    \"\"\"\n",
    "    Converts a list of word indices into a human-readable sentence using a dictionary mapping indices to words.\n",
    "\n",
    "    Args:\n",
    "        ids (List[int]): A list of integer word indices.\n",
    "        idx_dict (Dict[int, str]): A dictionary mapping word indices to their corresponding words.\n",
    "\n",
    "    Returns:\n",
    "        str: The reconstructed sentence from the list of indices, with appropriate spacing and punctuation.\n",
    "    \"\"\"\n",
    "    # Convert index list to words, defaulting to \"UNK\" for missing indices\n",
    "    tokens = [idx_dict.get(i, \"UNK\") for i in ids]\n",
    "    \n",
    "    # Join tokens into a string and replace special tokens with spaces\n",
    "    sentence = \"\".join(tokens).replace(\"</w>\", \" \")\n",
    "\n",
    "    # Remove spaces before punctuation\n",
    "    punctuation = '''?:;.,'(\"-!&)%'''\n",
    "    for punct in punctuation:\n",
    "        sentence = sentence.replace(f\" {punct}\", punct)\n",
    "\n",
    "    # Return the cleaned sentence, ensuring no trailing whitespace\n",
    "    return sentence.rstrip()\n",
    "\n",
    "\n",
    "def seq_padding(X, padding=0):\n",
    "    \"\"\"\n",
    "    Pads each sequence in the input list to the length of the longest sequence.\n",
    "\n",
    "    This function takes a list of sequences (lists or arrays) and pads each sequence\n",
    "    with a specified value (default is 0) so that all sequences have the same length. \n",
    "    The length of the padded sequences will be equal to the length of the longest sequence\n",
    "    found in the input list.\n",
    "\n",
    "    Parameters:\n",
    "    X (list of list/array): A list containing sequences (lists or numpy arrays).\n",
    "    padding (int, optional): The value to use for padding shorter sequences. Default is 0.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: A 2D numpy array where each row corresponds to a padded sequence.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the lengths of each sequence\n",
    "    lengths = [len(x) for x in X]\n",
    "    \n",
    "    # Determine the maximum length\n",
    "    max_length = max(lengths)\n",
    "    \n",
    "    # Pad sequences to the maximum length\n",
    "    padded_sequences = np.array([\n",
    "        np.concatenate([x, [padding] * (max_length - len(x))]) if len(x) < max_length else x \n",
    "        for x in X\n",
    "    ])\n",
    "    \n",
    "    return padded_sequences\n",
    "\n",
    "class Batch:\n",
    "    \"\"\"\n",
    "    A class to represent a batch of source and target sequences for processing in a neural network.\n",
    "\n",
    "    The Batch class handles the conversion of numpy arrays to PyTorch tensors and creates masks for the\n",
    "    source and target sequences. It also computes the number of tokens in the target sequences that \n",
    "    are not padding.\n",
    "\n",
    "    Attributes:\n",
    "        src (torch.Tensor): The source sequences as a tensor.\n",
    "        src_mask (torch.Tensor): A mask indicating the positions of the valid tokens in the source sequences.\n",
    "        trg (torch.Tensor, optional): The target sequences (excluding the last token).\n",
    "        trg_y (torch.Tensor, optional): The target sequences (excluding the first token).\n",
    "        trg_mask (torch.Tensor, optional): A mask indicating the positions of valid tokens in the target sequences.\n",
    "        ntokens (int, optional): The number of valid tokens in the target sequences.\n",
    "\n",
    "    Parameters:\n",
    "        src (numpy.ndarray): A numpy array representing the source sequences.\n",
    "        trg (numpy.ndarray, optional): A numpy array representing the target sequences. Defaults to None.\n",
    "        pad (int, optional): The padding token. Defaults to 0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        \n",
    "        # Convert numpy arrays to PyTorch tensors and move to the specified device\n",
    "        self.src = torch.from_numpy(src).to(DEVICE).long()\n",
    "        trg = torch.from_numpy(trg).to(DEVICE).long()\n",
    "        \n",
    "        # Create a mask for the source sequences\n",
    "        self.src_mask = (self.src != pad).unsqueeze(-2)\n",
    "\n",
    "        if trg is not None:\n",
    "            # Convert target sequences to tensor and exclude the last token for input\n",
    "            self.trg = trg[:, :-1]\n",
    "            \n",
    "            # Exclude the first token for output\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            \n",
    "            # Create a mask for the target sequences\n",
    "            self.trg_mask = make_std_mask(self.trg, pad)\n",
    "            \n",
    "            # Count the number of valid tokens in the target sequences\n",
    "            self.ntokens = (self.trg_y != pad).sum().item()\n",
    "            \n",
    "            \n",
    "def subsequent_mask(sequence_length):\n",
    "    \"\"\"\n",
    "    Generates a subsequent mask for self-attention in transformer models.\n",
    "\n",
    "    This function creates a mask that prevents attention to future tokens in the sequence. \n",
    "    The mask is used in self-attention layers to ensure that each position can only attend\n",
    "    to itself and the previous positions, enforcing an autoregressive property.\n",
    "\n",
    "    Parameters:\n",
    "        sequence_length (int): The length of the input sequence for which to generate the mask.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (1, sequence_length, sequence_length) where positions\n",
    "                      corresponding to future tokens are masked (set to False) and positions \n",
    "                      corresponding to the current and past tokens are unmasked (set to True).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a triangular mask with ones above the main diagonal\n",
    "    mask_shape = (1, sequence_length, sequence_length)\n",
    "    upper_triangle_mask = np.triu(np.ones(mask_shape), k=1).astype('uint8')\n",
    "    \n",
    "    # Convert the numpy array to a PyTorch tensor and create a boolean mask\n",
    "    boolean_mask = torch.from_numpy(upper_triangle_mask) == 0\n",
    "    \n",
    "    return boolean_mask\n",
    "\n",
    "def make_std_mask(target_tensor, pad_token):\n",
    "    \"\"\"\n",
    "    Creates a standard mask for the target sequences in transformer models.\n",
    "\n",
    "    This function generates a mask that allows attention to valid tokens in the target \n",
    "    sequence while masking out padding tokens and future tokens. The resulting mask can \n",
    "    be used in self-attention layers to ensure proper behavior during training and inference.\n",
    "\n",
    "    Parameters:\n",
    "        target_tensor (torch.Tensor): The tensor representing the target sequences.\n",
    "        pad_token (int): The token value used for padding in the target sequences.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A boolean mask of shape (batch_size, 1, target_length) where valid tokens are\n",
    "                      marked as True and masked tokens (padding and future tokens) are marked as False.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a mask to identify non-padding tokens\n",
    "    valid_token_mask = (target_tensor != pad_token).unsqueeze(-2)\n",
    "    \n",
    "    # Create a subsequent mask to prevent attention to future tokens\n",
    "    future_token_mask = subsequent_mask(target_tensor.size(-1)).type_as(valid_token_mask.data)\n",
    "    \n",
    "    # Combine the valid token mask with the future token mask\n",
    "    combined_mask = valid_token_mask & future_token_mask\n",
    "    \n",
    "    return combined_mask\n",
    "\n",
    "def create_batches(english_word_dict, english_tokens, french_word_dict, french_tokens):\n",
    "    \"\"\"\n",
    "    Creates batches of tokenized English and French sentences for training.\n",
    "\n",
    "    This function converts lists of tokenized sentences into numerical IDs using the provided \n",
    "    word dictionaries. It then sorts the sentences by length, shuffles the indices, and \n",
    "    groups the sentences into batches of a specified size. Finally, it pads the sequences \n",
    "    within each batch to ensure uniformity.\n",
    "\n",
    "    Parameters:\n",
    "        english_word_dict (dict): A dictionary mapping English words to their corresponding IDs.\n",
    "        english_tokens (list of list of str): A list of tokenized English sentences.\n",
    "        french_word_dict (dict): A dictionary mapping French words to their corresponding IDs.\n",
    "        french_tokens (list of list of str): A list of tokenized French sentences.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Batch objects, each containing padded English and French sequences.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert English tokens to IDs using the provided dictionary, using 1 for unknown words\n",
    "    english_ids = [[english_word_dict.get(word, 1) for word in sentence] for sentence in english_tokens]\n",
    "    \n",
    "    # Convert French tokens to IDs using the provided dictionary, using 1 for unknown words\n",
    "    french_ids = [[french_word_dict.get(word, 1) for word in sentence] for sentence in french_tokens]\n",
    "    \n",
    "    # Sort the IDs by the length of the sentences\n",
    "    sorted_indices = sorted(range(len(english_ids)), key=lambda index: len(english_ids[index]))\n",
    "    english_ids = [english_ids[index] for index in sorted_indices]\n",
    "    french_ids = [french_ids[index] for index in sorted_indices]\n",
    "\n",
    "    # Create a list of shuffled indices for batching\n",
    "    index_list = np.arange(0, len(english_tokens), BATCH_SIZE)\n",
    "    np.random.shuffle(index_list)\n",
    "\n",
    "    batch_indices = []\n",
    "    for index in index_list:\n",
    "        batch_indices.append(np.arange(index, min(len(english_tokens), index + BATCH_SIZE)))\n",
    "\n",
    "    batches = []\n",
    "    for indices in batch_indices:\n",
    "        batch_english = [english_ids[index] for index in indices]\n",
    "        batch_french = [french_ids[index] for index in indices]\n",
    "        \n",
    "        # Pad the sequences to the maximum length in the batch\n",
    "        padded_batch_english = seq_padding(batch_english)\n",
    "        padded_batch_french = seq_padding(batch_french)\n",
    "        \n",
    "        # Create a Batch object with the padded sequences\n",
    "        batches.append(Batch(padded_batch_english, padded_batch_french))\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    A class to create word embeddings using PyTorch's nn.Embedding.\n",
    "\n",
    "    This class generates dense vector representations for words in a vocabulary. \n",
    "    It initializes an embedding layer and scales the output by the square root of the model dimension.\n",
    "\n",
    "    Attributes:\n",
    "        embedding_layer (nn.Embedding): The embedding layer mapping vocabulary indices to dense vectors.\n",
    "        model_dimension (int): The dimensionality of the embedding space.\n",
    "    \n",
    "    Parameters:\n",
    "        d_model (int): The size of each embedding vector.\n",
    "        vocab_size (int): The number of unique tokens in the vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_dimension, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, model_dimension)\n",
    "        self.model_dimension = model_dimension\n",
    "\n",
    "    def forward(self, input_indices):\n",
    "        \"\"\"\n",
    "        Forward pass for generating embeddings.\n",
    "\n",
    "        This method takes a tensor of input indices and retrieves their corresponding \n",
    "        embedding vectors from the embedding layer. The output is scaled by the square \n",
    "        root of the model dimension to stabilize gradients.\n",
    "\n",
    "        Parameters:\n",
    "            input_indices (torch.Tensor): A tensor containing the indices of the words to be embedded.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of shape (batch_size, sequence_length, d_model) containing \n",
    "                          the scaled embeddings.\n",
    "        \"\"\"\n",
    "        embeddings = self.embedding_layer(input_indices) * math.sqrt(self.model_dimension)\n",
    "        return embeddings\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    A class to apply positional encoding to input sequences.\n",
    "\n",
    "    Positional encoding is used in transformer models to provide information about the position of each token\n",
    "    in the sequence. This implementation generates sinusoidal positional encodings and applies dropout for regularization.\n",
    "\n",
    "    Attributes:\n",
    "        dropout_layer (nn.Dropout): A dropout layer for regularization.\n",
    "        positional_encodings (torch.Tensor): Precomputed positional encodings.\n",
    "\n",
    "    Parameters:\n",
    "        model_dimension (int): The size of each embedding vector (d_model).\n",
    "        dropout_prob (float): The dropout probability for regularization.\n",
    "        max_length (int, optional): The maximum length of input sequences. Defaults to 5000.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_dimension, dropout_prob, max_length=5000):\n",
    "        super().__init__()\n",
    "        self.dropout_layer = nn.Dropout(p=dropout_prob)\n",
    "        \n",
    "        # Create a tensor for positional encodings\n",
    "        positional_encodings = torch.zeros(max_length, model_dimension, device=DEVICE)\n",
    "        \n",
    "        # Compute the positional indices\n",
    "        position = torch.arange(0., max_length, device=DEVICE).unsqueeze(1)\n",
    "\n",
    "        # Calculate the divisor term for sine and cosine functions\n",
    "        div_term = torch.exp(torch.arange(0., model_dimension, 2, device=DEVICE) * \n",
    "                             -(math.log(10000.0) / model_dimension))\n",
    "        \n",
    "        # Compute the positional encodings using sine and cosine functions\n",
    "        positional_encodings_pos = torch.mul(position, div_term)\n",
    "        positional_encodings[:, 0::2] = torch.sin(positional_encodings_pos)\n",
    "        positional_encodings[:, 1::2] = torch.cos(positional_encodings_pos)\n",
    "        \n",
    "        # Add a batch dimension\n",
    "        positional_encodings = positional_encodings.unsqueeze(0)\n",
    "        \n",
    "        # Register positional encodings as a persistent buffer\n",
    "        self.register_buffer('positional_encodings', positional_encodings)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        \"\"\"\n",
    "        Forward pass to add positional encodings to input sequences.\n",
    "\n",
    "        This method adds the precomputed positional encodings to the input tensor, \n",
    "        scales it with dropout for regularization, and returns the resulting tensor.\n",
    "\n",
    "        Parameters:\n",
    "            input_tensor (torch.Tensor): A tensor containing the input embeddings of shape \n",
    "                                          (batch_size, sequence_length, d_model).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of the same shape as input_tensor, with positional encodings added.\n",
    "        \"\"\"\n",
    "        # Add positional encodings to the input tensor\n",
    "        input_tensor = input_tensor + self.positional_encodings[:, :input_tensor.size(1)].requires_grad_(False)\n",
    "        \n",
    "        # Apply dropout\n",
    "        output_tensor = self.dropout_layer(input_tensor)\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384977de",
   "metadata": {},
   "source": [
    "## 1: Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89b624fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokens, en_word_dict, en_idx_dict = create_dictionary(data, tokenizer, language='en')\n",
    "fr_tokens, fr_word_dict, fr_idx_dict = create_dictionary(data, tokenizer, language='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9a3a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_en = tokenizer.tokenize(\"how are you?\")\n",
    "tokenized_fr = tokenizer.tokenize(\"comment etes-vous?\")\n",
    "\n",
    "ids_en = [en_word_dict.get(i,UNK) for i in tokenized_en] \n",
    "ids_fr = [fr_word_dict.get(i,UNK) for i in tokenized_fr] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec6b6b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how are you?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_sentence(ids_en, en_idx_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0c0bc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comment etes-vous?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_sentence(ids_fr, fr_idx_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a26567",
   "metadata": {},
   "source": [
    "## 2: Batch Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88a02348",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = create_batches(en_word_dict, en_tokens, fr_word_dict, fr_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cdb910",
   "metadata": {},
   "source": [
    "## 3: Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "974493c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 11055 distinct En tokens.\n",
      "There are 11239 distinct Fr tokens.\n"
     ]
    }
   ],
   "source": [
    "src_vocab = len(en_word_dict)\n",
    "tgt_vocab = len(fr_word_dict)\n",
    "print(f\"There are {src_vocab} distinct En tokens.\")\n",
    "print(f\"There are {tgt_vocab} distinct Fr tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dd45276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of positional encoding is torch.Size([1, 8, 256])\n"
     ]
    }
   ],
   "source": [
    "example = PositionalEncoding(256, 0.1)\n",
    "inputs = torch.zeros(1, 8, 256).to(DEVICE)\n",
    "outputs = example.forward(inputs)\n",
    "print(f\"The shape of positional encoding is {outputs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5fe88d",
   "metadata": {},
   "source": [
    "## 4: Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b8c9512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query: torch.Tensor, \n",
    "              key: torch.Tensor, \n",
    "              value: torch.Tensor, \n",
    "              mask: Optional[torch.Tensor] = None, \n",
    "              dropout_layer: Optional[Dropout] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Computes the scaled dot-product attention.\n",
    "\n",
    "    This function calculates the attention scores by performing a dot product between \n",
    "    the query and key tensors, applies an optional mask to the scores, and generates \n",
    "    the attention weights. It then uses these weights to compute the weighted sum of the \n",
    "    value tensor. Dropout can also be applied to the attention weights for regularization.\n",
    "\n",
    "    Parameters:\n",
    "        query (torch.Tensor): The query tensor of shape (batch_size, num_heads, seq_length, d_k).\n",
    "        key (torch.Tensor): The key tensor of shape (batch_size, num_heads, seq_length, d_k).\n",
    "        value (torch.Tensor): The value tensor of shape (batch_size, num_heads, seq_length, d_v).\n",
    "        mask (Optional[torch.Tensor], optional): A mask tensor to prevent attention to certain positions.\n",
    "                                                  Should have shape (batch_size, 1, seq_length, seq_length).\n",
    "        dropout_layer (Optional[Dropout], optional): A dropout layer to apply to the attention weights.\n",
    "                                                      If None, no dropout is applied.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: A tuple containing:\n",
    "            - The output tensor of shape (batch_size, num_heads, seq_length, d_v) which is the result of \n",
    "              the weighted sum of the value tensor.\n",
    "            - The attention weights tensor of shape (batch_size, num_heads, seq_length, seq_length).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the dimensionality of the keys\n",
    "    depth_key = query.size(-1)\n",
    "    \n",
    "    # Compute the attention scores\n",
    "    attention_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(depth_key)\n",
    "    \n",
    "    # Apply the mask if provided\n",
    "    if mask is not None:\n",
    "        attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # Calculate the attention weights\n",
    "    attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "    \n",
    "    # Apply dropout if specified\n",
    "    if dropout_layer is not None:\n",
    "        attention_weights = dropout_layer(attention_weights)\n",
    "    \n",
    "    # Compute the output as the weighted sum of the value tensor\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements multi-headed attention mechanism.\n",
    "\n",
    "    This class utilizes multiple attention heads to compute attention scores in parallel, \n",
    "    allowing the model to focus on different parts of the input sequence. The attention \n",
    "    outputs from each head are concatenated and linearly transformed.\n",
    "\n",
    "    Attributes:\n",
    "        num_heads (int): The number of attention heads.\n",
    "        depth_per_head (int): The dimensionality of each attention head.\n",
    "        linear_layers (nn.ModuleList): A list of linear transformation layers.\n",
    "        attention_weights (torch.Tensor, optional): The attention weights from the last forward pass.\n",
    "        dropout_layer (nn.Dropout): Dropout layer for regularization.\n",
    "\n",
    "    Parameters:\n",
    "        num_heads (int): The number of attention heads.\n",
    "        model_dimension (int): The dimensionality of the input and output (d_model).\n",
    "        dropout_prob (float): The dropout probability for regularization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads: int, model_dimension: int, dropout_prob: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert model_dimension % num_heads == 0, \"Model dimension must be divisible by the number of heads.\"\n",
    "        \n",
    "        self.depth_per_head = model_dimension // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Create linear layers for query, key, value, and output transformations\n",
    "        self.linear_layers = nn.ModuleList([deepcopy(nn.Linear(model_dimension, model_dimension)) for _ in range(4)])\n",
    "        self.attention_weights = None\n",
    "        self.dropout_layer = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for multi-headed attention.\n",
    "\n",
    "        This method computes the attention output for the provided query, key, and value tensors.\n",
    "        It applies the attention mechanism, utilizes an optional mask, and combines the results\n",
    "        from multiple attention heads.\n",
    "\n",
    "        Parameters:\n",
    "            query (torch.Tensor): The query tensor of shape (batch_size, seq_length, d_model).\n",
    "            key (torch.Tensor): The key tensor of shape (batch_size, seq_length, d_model).\n",
    "            value (torch.Tensor): The value tensor of shape (batch_size, seq_length, d_model).\n",
    "            mask (Optional[torch.Tensor]): An optional mask tensor to prevent attending to certain positions.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after applying multi-headed attention of shape \n",
    "                          (batch_size, seq_length, d_model).\n",
    "        \"\"\"\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)  # Add a dimension for multi-head attention\n",
    "\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # Apply linear transformations and reshape for multi-head attention\n",
    "        query, key, value = [\n",
    "            linear_layer(x).view(batch_size, -1, self.num_heads, self.depth_per_head).transpose(1, 2)\n",
    "            for linear_layer, x in zip(self.linear_layers, (query, key, value))\n",
    "        ]\n",
    "\n",
    "        # Calculate attention using the helper function\n",
    "        attention_output, self.attention_weights = attention(query, key, value, mask=mask, dropout_layer=self.dropout_layer)\n",
    "        \n",
    "        # Reshape and concatenate the outputs from all heads\n",
    "        output = attention_output.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.depth_per_head)\n",
    "        \n",
    "        # Apply the final linear transformation\n",
    "        output = self.linear_layers[-1](output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a position-wise feed-forward neural network.\n",
    "\n",
    "    This class represents a feed-forward network used in transformer models. \n",
    "    It consists of two linear transformations with a dropout layer in between. \n",
    "    The first linear transformation increases the dimensionality, while the second \n",
    "    reduces it back to the original model dimension.\n",
    "\n",
    "    Attributes:\n",
    "        linear_layer1 (nn.Linear): The first linear transformation layer.\n",
    "        linear_layer2 (nn.Linear): The second linear transformation layer.\n",
    "        dropout_layer (nn.Dropout): Dropout layer for regularization.\n",
    "\n",
    "    Parameters:\n",
    "        model_dimension (int): The dimensionality of the input (d_model).\n",
    "        feedforward_dimension (int): The dimensionality of the feed-forward layer (d_ff).\n",
    "        dropout_prob (float): The dropout probability for regularization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_dimension: int, feedforward_dimension: int, dropout_prob: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear_layer1 = nn.Linear(model_dimension, feedforward_dimension)\n",
    "        self.linear_layer2 = nn.Linear(feedforward_dimension, model_dimension)\n",
    "        self.dropout_layer = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the position-wise feed-forward network.\n",
    "\n",
    "        This method takes an input tensor and applies two linear transformations \n",
    "        with a dropout layer in between.\n",
    "\n",
    "        Parameters:\n",
    "            input_tensor (torch.Tensor): The input tensor of shape (batch_size, seq_length, d_model).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of the same shape as input_tensor, after applying \n",
    "                          the feed-forward network transformations.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Apply the first linear transformation\n",
    "        intermediate_output = self.linear_layer1(input_tensor)\n",
    "        \n",
    "        # Apply dropout to the intermediate output\n",
    "        dropped_output = self.dropout_layer(intermediate_output)\n",
    "        \n",
    "        # Apply the second linear transformation\n",
    "        output_tensor = self.linear_layer2(dropped_output)\n",
    "        \n",
    "        return output_tensor\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the encoder part of a transformer model.\n",
    "\n",
    "    This class consists of multiple layers of the transformer encoder, \n",
    "    each of which applies a self-attention mechanism followed by \n",
    "    a feed-forward neural network. A layer normalization is applied \n",
    "    at the end of the encoder.\n",
    "\n",
    "    Attributes:\n",
    "        layers (nn.ModuleList): A list containing multiple encoder layers.\n",
    "        layer_norm (nn.LayerNorm): Layer normalization applied to the output.\n",
    "\n",
    "    Parameters:\n",
    "        encoder_layer (nn.Module): A single encoder layer instance.\n",
    "        num_layers (int): The number of encoder layers in the encoder stack.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_layer: nn.Module, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([deepcopy(encoder_layer) for _ in range(num_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(encoder_layer.layer_size)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the encoder.\n",
    "\n",
    "        This method applies each encoder layer sequentially to the input tensor \n",
    "        and then applies layer normalization to the final output.\n",
    "\n",
    "        Parameters:\n",
    "            input_tensor (torch.Tensor): The input tensor of shape (batch_size, seq_length, d_model).\n",
    "            mask (torch.Tensor): A mask tensor to prevent attending to certain positions.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The normalized output tensor after passing through all encoder layers.\n",
    "        \"\"\"\n",
    "        \n",
    "        for encoder_layer in self.layers:\n",
    "            input_tensor = encoder_layer(input_tensor, mask)\n",
    "        \n",
    "        output_tensor = self.layer_norm(input_tensor)\n",
    "        return output_tensor\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the decoder part of a transformer model.\n",
    "\n",
    "    This class consists of multiple layers of the transformer decoder,\n",
    "    each applying self-attention and encoder-decoder attention mechanisms.\n",
    "    A layer normalization is applied to the output of the decoder.\n",
    "\n",
    "    Attributes:\n",
    "        layers (nn.ModuleList): A list containing multiple decoder layers.\n",
    "        layer_norm (nn.LayerNorm): Layer normalization applied to the output.\n",
    "\n",
    "    Parameters:\n",
    "        decoder_layer (nn.Module): A single decoder layer instance.\n",
    "        num_layers (int): The number of decoder layers in the decoder stack.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, decoder_layer: nn.Module, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([deepcopy(decoder_layer) for _ in range(num_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(decoder_layer.layer_size)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, memory_tensor: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the decoder.\n",
    "\n",
    "        This method applies each decoder layer sequentially to the input tensor \n",
    "        using the provided memory tensor, source mask, and target mask. \n",
    "        Layer normalization is applied to the final output.\n",
    "\n",
    "        Parameters:\n",
    "            input_tensor (torch.Tensor): The input tensor of shape (batch_size, seq_length, d_model).\n",
    "            memory_tensor (torch.Tensor): The output from the encoder to attend to.\n",
    "            src_mask (torch.Tensor): A mask tensor to prevent attending to certain positions in the source.\n",
    "            tgt_mask (torch.Tensor): A mask tensor to prevent attending to certain positions in the target.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The normalized output tensor after passing through all decoder layers.\n",
    "        \"\"\"\n",
    "        \n",
    "        for decoder_layer in self.layers:\n",
    "            input_tensor = decoder_layer(input_tensor, memory_tensor, src_mask, tgt_mask)\n",
    "        \n",
    "        output_tensor = self.layer_norm(input_tensor)\n",
    "        return output_tensor\n",
    "    \n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a sublayer connection for transformer models.\n",
    "\n",
    "    This class combines the output of a sublayer (like self-attention or feed-forward layers)\n",
    "    with the input tensor through a residual connection followed by layer normalization and dropout.\n",
    "\n",
    "    Attributes:\n",
    "        layer_norm (nn.LayerNorm): Layer normalization applied to the input.\n",
    "        dropout_layer (nn.Dropout): Dropout layer for regularization.\n",
    "\n",
    "    Parameters:\n",
    "        size (int): The dimensionality of the input tensor (d_model).\n",
    "        dropout_prob (float): The dropout probability for regularization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size: int, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(size)\n",
    "        self.dropout_layer = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, sublayer: Callable[[torch.Tensor], torch.Tensor]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the sublayer connection.\n",
    "\n",
    "        This method applies layer normalization to the input tensor, \n",
    "        passes it through a sublayer, applies dropout, and then \n",
    "        adds the original input tensor (residual connection) \n",
    "        to produce the output tensor.\n",
    "\n",
    "        Parameters:\n",
    "            input_tensor (torch.Tensor): The input tensor of shape (batch_size, seq_length, d_model).\n",
    "            sublayer (Callable[[torch.Tensor], torch.Tensor]): A function that takes a tensor as input\n",
    "                and returns a tensor as output, representing the sublayer operation.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after applying the sublayer connection.\n",
    "        \"\"\"\n",
    "        \n",
    "        normalized_input = self.layer_norm(input_tensor)\n",
    "        sublayer_output = sublayer(normalized_input)\n",
    "        dropped_output = self.dropout_layer(sublayer_output)\n",
    "        output_tensor = input_tensor + dropped_output\n",
    "        \n",
    "        return output_tensor\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a single layer of the transformer encoder.\n",
    "\n",
    "    This class contains a self-attention mechanism followed by a feed-forward network. \n",
    "    Each operation is wrapped in a sublayer connection to apply layer normalization \n",
    "    and residual connections.\n",
    "\n",
    "    Attributes:\n",
    "        self_attention (nn.Module): The self-attention mechanism for this layer.\n",
    "        feed_forward_network (nn.Module): The feed-forward network for this layer.\n",
    "        sublayer_connections (nn.ModuleList): A list of sublayer connections for the self-attention \n",
    "                                               and feed-forward operations.\n",
    "        layer_size (int): The dimensionality of the input and output tensors.\n",
    "\n",
    "    Parameters:\n",
    "        layer_size (int): The dimensionality of the input tensor (d_model).\n",
    "        self_attention (nn.Module): An instance of the self-attention mechanism.\n",
    "        feed_forward_network (nn.Module): An instance of the feed-forward network.\n",
    "        dropout_prob (float): The dropout probability for regularization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer_size: int, self_attention: nn.Module, feed_forward_network: nn.Module, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.self_attention = self_attention\n",
    "        self.feed_forward_network = feed_forward_network\n",
    "        self.sublayer_connections = nn.ModuleList([\n",
    "            deepcopy(SublayerConnection(layer_size, dropout_prob)) for _ in range(2)\n",
    "        ])\n",
    "        self.layer_size = layer_size\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the encoder layer.\n",
    "\n",
    "        This method applies self-attention to the input tensor, followed by a \n",
    "        feed-forward network. Each operation is wrapped in a sublayer connection \n",
    "        to ensure proper normalization and residual connections.\n",
    "\n",
    "        Parameters:\n",
    "            input_tensor (torch.Tensor): The input tensor of shape (batch_size, seq_length, d_model).\n",
    "            mask (torch.Tensor): A mask tensor to prevent attending to certain positions.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after processing through the encoder layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Apply self-attention with residual connection\n",
    "        attended_output = self.sublayer_connections[0](\n",
    "            input_tensor, lambda x: self.self_attention(x, x, x, mask)\n",
    "        )\n",
    "        \n",
    "        # Apply feed-forward network with residual connection\n",
    "        output_tensor = self.sublayer_connections[1](attended_output, self.feed_forward_network)\n",
    "        \n",
    "        return output_tensor\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a single layer of the transformer decoder.\n",
    "\n",
    "    This class contains mechanisms for self-attention and encoder-decoder attention,\n",
    "    followed by a feed-forward network. Each operation is wrapped in a sublayer connection\n",
    "    to apply layer normalization and residual connections.\n",
    "\n",
    "    Attributes:\n",
    "        layer_size (int): The dimensionality of the input and output tensors (d_model).\n",
    "        self_attention (nn.Module): The self-attention mechanism for this layer.\n",
    "        source_attention (nn.Module): The attention mechanism that attends to encoder outputs.\n",
    "        feed_forward_network (nn.Module): The feed-forward network for this layer.\n",
    "        sublayer_connections (nn.ModuleList): A list of sublayer connections for self-attention,\n",
    "                                               source attention, and feed-forward operations.\n",
    "\n",
    "    Parameters:\n",
    "        layer_size (int): The dimensionality of the input tensor (d_model).\n",
    "        self_attention (nn.Module): An instance of the self-attention mechanism.\n",
    "        source_attention (nn.Module): An instance of the encoder-decoder attention mechanism.\n",
    "        feed_forward_network (nn.Module): An instance of the feed-forward network.\n",
    "        dropout_prob (float): The dropout probability for regularization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer_size: int, self_attention: nn.Module, source_attention: nn.Module,\n",
    "                 feed_forward_network: nn.Module, dropout_prob: float):\n",
    "        super().__init__()\n",
    "        self.layer_size = layer_size\n",
    "        self.self_attention = self_attention\n",
    "        self.source_attention = source_attention\n",
    "        self.feed_forward_network = feed_forward_network\n",
    "        self.sublayer_connections = nn.ModuleList([\n",
    "            deepcopy(SublayerConnection(layer_size, dropout_prob)) for _ in range(3)\n",
    "        ])\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor, memory_tensor: torch.Tensor, \n",
    "                src_mask: torch.Tensor, tgt_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the decoder layer.\n",
    "\n",
    "        This method applies self-attention to the input tensor, followed by \n",
    "        attention to the encoder output (memory), and finally applies a \n",
    "        feed-forward network. Each operation is wrapped in a sublayer \n",
    "        connection to ensure proper normalization and residual connections.\n",
    "\n",
    "        Parameters:\n",
    "            input_tensor (torch.Tensor): The input tensor of shape (batch_size, seq_length, d_model).\n",
    "            memory_tensor (torch.Tensor): The output from the encoder to attend to.\n",
    "            src_mask (torch.Tensor): A mask tensor to prevent attending to certain positions in the source.\n",
    "            tgt_mask (torch.Tensor): A mask tensor to prevent attending to certain positions in the target.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after processing through the decoder layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Apply self-attention with residual connection\n",
    "        attended_output = self.sublayer_connections[0](\n",
    "            input_tensor, lambda x: self.self_attention(x, x, x, tgt_mask)\n",
    "        )\n",
    "        \n",
    "        # Apply source attention with residual connection\n",
    "        attended_output = self.sublayer_connections[1](\n",
    "            attended_output, lambda x: self.source_attention(x, memory_tensor, memory_tensor, src_mask)\n",
    "        )\n",
    "        \n",
    "        # Apply feed-forward network with residual connection\n",
    "        output_tensor = self.sublayer_connections[2](attended_output, self.feed_forward_network)\n",
    "        \n",
    "        return output_tensor\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Transformer architecture as described in \"Attention is All You Need\".\n",
    "\n",
    "    This class consists of an encoder and a decoder, with embedding layers for \n",
    "    source and target sequences, as well as a generator for output predictions.\n",
    "\n",
    "    Attributes:\n",
    "        encoder (nn.Module): The encoder component of the Transformer.\n",
    "        decoder (nn.Module): The decoder component of the Transformer.\n",
    "        source_embedding (nn.Module): The embedding layer for source sequences.\n",
    "        target_embedding (nn.Module): The embedding layer for target sequences.\n",
    "        generator (nn.Module): The final layer for generating predictions from the decoder output.\n",
    "\n",
    "    Parameters:\n",
    "        encoder (nn.Module): An instance of the encoder module.\n",
    "        decoder (nn.Module): An instance of the decoder module.\n",
    "        source_embedding (nn.Module): An instance of the source embedding layer.\n",
    "        target_embedding (nn.Module): An instance of the target embedding layer.\n",
    "        generator (nn.Module): An instance of the generator layer for producing output.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder: nn.Module, decoder: nn.Module,\n",
    "                 source_embedding: nn.Module, target_embedding: nn.Module, \n",
    "                 generator: nn.Module):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.source_embedding = source_embedding\n",
    "        self.target_embedding = target_embedding\n",
    "        self.generator = generator\n",
    "\n",
    "    def encode(self, source: torch.Tensor, source_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encodes the source sequences using the encoder.\n",
    "\n",
    "        This method applies the source embedding followed by the encoder.\n",
    "\n",
    "        Parameters:\n",
    "            source (torch.Tensor): The input source tensor of shape (batch_size, src_seq_length).\n",
    "            source_mask (torch.Tensor): A mask tensor to prevent attending to certain positions in the source.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The encoded representation of the source sequences.\n",
    "        \"\"\"\n",
    "        embedded_source = self.source_embedding(source)\n",
    "        return self.encoder(embedded_source, source_mask)\n",
    "\n",
    "    def decode(self, memory: torch.Tensor, source_mask: torch.Tensor, \n",
    "               target: torch.Tensor, target_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Decodes the target sequences using the decoder.\n",
    "\n",
    "        This method applies the target embedding followed by the decoder using the encoder's output as memory.\n",
    "\n",
    "        Parameters:\n",
    "            memory (torch.Tensor): The encoded representation from the encoder.\n",
    "            source_mask (torch.Tensor): A mask tensor for the source.\n",
    "            target (torch.Tensor): The input target tensor of shape (batch_size, tgt_seq_length).\n",
    "            target_mask (torch.Tensor): A mask tensor to prevent attending to certain positions in the target.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after decoding the target sequences.\n",
    "        \"\"\"\n",
    "        embedded_target = self.target_embedding(target)\n",
    "        return self.decoder(embedded_target, memory, source_mask, target_mask)\n",
    "\n",
    "    def forward(self, source: torch.Tensor, target: torch.Tensor, \n",
    "                source_mask: torch.Tensor, target_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the Transformer.\n",
    "\n",
    "        This method encodes the source sequences and then decodes the target sequences\n",
    "        to produce the final output.\n",
    "\n",
    "        Parameters:\n",
    "            source (torch.Tensor): The input source tensor of shape (batch_size, src_seq_length).\n",
    "            target (torch.Tensor): The input target tensor of shape (batch_size, tgt_seq_length).\n",
    "            source_mask (torch.Tensor): A mask tensor for the source.\n",
    "            target_mask (torch.Tensor): A mask tensor for the target.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after processing the source and target sequences.\n",
    "        \"\"\"\n",
    "        memory = self.encode(source, source_mask)\n",
    "        output = self.decode(memory, source_mask, target, target_mask)\n",
    "        return output\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    A linear projection layer followed by a log softmax activation.\n",
    "\n",
    "    This class is used to convert the output of the Transformer model into \n",
    "    a probability distribution over the target vocabulary.\n",
    "\n",
    "    Attributes:\n",
    "        projection_layer (nn.Linear): A linear layer that projects the model output to the vocabulary size.\n",
    "\n",
    "    Parameters:\n",
    "        model_dimension (int): The dimensionality of the model (d_model).\n",
    "        vocabulary_size (int): The size of the target vocabulary.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_dimension: int, vocabulary_size: int):\n",
    "        super().__init__()\n",
    "        self.projection_layer = nn.Linear(model_dimension, vocabulary_size)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the generator.\n",
    "\n",
    "        This method applies a linear transformation to the input tensor and \n",
    "        then applies the log softmax function to produce probabilities.\n",
    "\n",
    "        Parameters:\n",
    "            input_tensor (torch.Tensor): The input tensor of shape (batch_size, seq_length, d_model).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor containing the log probabilities of shape (batch_size, seq_length, vocab_size).\n",
    "        \"\"\"\n",
    "        linear_output = self.projection_layer(input_tensor)\n",
    "        log_probs = nn.functional.log_softmax(linear_output, dim=-1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c1a898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformer_model(source_vocab_size: int, target_vocab_size: int, \n",
    "                              num_layers: int, model_dimension: int,\n",
    "                              feed_forward_dimension: int, num_heads: int, \n",
    "                              dropout_rate: float = 0.1) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Creates a Transformer model for sequence-to-sequence tasks.\n",
    "\n",
    "    This function initializes all the components of the Transformer architecture,\n",
    "    including multi-headed attention, position-wise feed-forward networks, \n",
    "    positional encodings, an encoder, a decoder, and a generator for output.\n",
    "\n",
    "    Parameters:\n",
    "        source_vocab_size (int): The size of the source vocabulary.\n",
    "        target_vocab_size (int): The size of the target vocabulary.\n",
    "        num_layers (int): The number of encoder and decoder layers.\n",
    "        model_dimension (int): The dimensionality of the model (d_model).\n",
    "        feed_forward_dimension (int): The dimensionality of the feed-forward network (d_ff).\n",
    "        num_heads (int): The number of attention heads.\n",
    "        dropout_rate (float, optional): The dropout rate. Defaults to 0.1.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: An instance of the Transformer model.\n",
    "    \"\"\"\n",
    "    # Initialize components of the Transformer\n",
    "    multi_head_attention = MultiHeadedAttention(num_heads, model_dimension).to(DEVICE)\n",
    "    feed_forward_network = PositionwiseFeedForward(model_dimension, feed_forward_dimension, dropout_rate).to(DEVICE)\n",
    "    positional_encoding = PositionalEncoding(model_dimension, dropout_rate).to(DEVICE)\n",
    "\n",
    "    # Build the Encoder and Decoder\n",
    "    encoder = Encoder(EncoderLayer(model_dimension, deepcopy(multi_head_attention), \n",
    "                                    deepcopy(feed_forward_network), dropout_rate).to(DEVICE), \n",
    "                      num_layers).to(DEVICE)\n",
    "\n",
    "    decoder = Decoder(DecoderLayer(model_dimension, deepcopy(multi_head_attention), \n",
    "                                    deepcopy(multi_head_attention), \n",
    "                                    deepcopy(feed_forward_network), dropout_rate).to(DEVICE), \n",
    "                      num_layers).to(DEVICE)\n",
    "\n",
    "    # Create embeddings and combine with positional encodings\n",
    "    source_embedding = nn.Sequential(Embeddings(model_dimension, source_vocab_size).to(DEVICE), \n",
    "                                      deepcopy(positional_encoding))\n",
    "    target_embedding = nn.Sequential(Embeddings(model_dimension, target_vocab_size).to(DEVICE), \n",
    "                                      deepcopy(positional_encoding))\n",
    "\n",
    "    # Create the final model\n",
    "    transformer_model = Transformer(encoder, decoder, source_embedding, target_embedding, \n",
    "                                     Generator(model_dimension, target_vocab_size)).to(DEVICE)\n",
    "\n",
    "    # Initialize model parameters\n",
    "    for parameter in transformer_model.parameters():\n",
    "        if parameter.dim() > 1:\n",
    "            nn.init.xavier_uniform_(parameter)\n",
    "\n",
    "    return transformer_model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8893bec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_transformer_model(source_vocab_size=src_vocab, \n",
    "                                 target_vocab_size=tgt_vocab, \n",
    "                                 num_layers=6, \n",
    "                                 model_dimension=256, \n",
    "                                 feed_forward_dimension=1024, \n",
    "                                 num_heads=8, \n",
    "                                 dropout_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6082d5bb",
   "metadata": {},
   "source": [
    "## 5: Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "480824e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements label smoothing to regularize the model by assigning a small amount of probability mass to \n",
    "    all other labels besides the correct one.\n",
    "    \n",
    "    Label smoothing helps in preventing overconfidence of the model by reducing the probability assigned to \n",
    "    the correct label and redistributing it across other classes.\n",
    "    \n",
    "    Attributes:\n",
    "        criterion (nn.KLDivLoss): The loss function used for calculating the Kullback-Leibler divergence between the smoothed distribution and the model output.\n",
    "        padding_index (int): The index in the vocabulary that represents padding, to be excluded from smoothing.\n",
    "        confidence (float): The confidence assigned to the correct label (1.0 - smoothing).\n",
    "        smoothing (float): The smoothing factor, which is the amount of probability mass distributed to other labels.\n",
    "        vocab_size (int): The size of the vocabulary or number of possible output classes.\n",
    "        true_distribution (Optional[torch.Tensor]): The smoothed true label distribution.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, padding_index: int, smoothing: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initializes the LabelSmoothing module.\n",
    "\n",
    "        Parameters:\n",
    "            vocab_size (int): The size of the output vocabulary (number of classes).\n",
    "            padding_index (int): The index used for padding in the target sequences.\n",
    "            smoothing (float, optional): The label smoothing factor. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction='sum')\n",
    "        self.padding_index = padding_index\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.vocab_size = vocab_size\n",
    "        self.true_distribution = None\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass to compute the loss with label smoothing.\n",
    "\n",
    "        Parameters:\n",
    "            logits (torch.Tensor): The model predictions (logits) of shape (batch_size, num_classes).\n",
    "            targets (torch.Tensor): The true target labels of shape (batch_size).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The computed loss value using smoothed target distribution.\n",
    "        \"\"\"\n",
    "        assert logits.size(1) == self.vocab_size, \"Logits and vocab size must match.\"\n",
    "\n",
    "        # Clone logits to create a smoothed distribution\n",
    "        smoothed_distribution = logits.data.clone()\n",
    "        smoothed_distribution.fill_(self.smoothing / (self.vocab_size - 2))\n",
    "        smoothed_distribution.scatter_(1, targets.data.unsqueeze(1), self.confidence)\n",
    "\n",
    "        # Set smoothing for padding tokens to zero\n",
    "        smoothed_distribution[:, self.padding_index] = 0\n",
    "        padding_mask = torch.nonzero(targets.data == self.padding_index)\n",
    "\n",
    "        if padding_mask.dim() > 0:\n",
    "            smoothed_distribution.index_fill_(0, padding_mask.squeeze(), 0.0)\n",
    "\n",
    "        self.true_distribution = smoothed_distribution  # Store for reference\n",
    "        loss = self.criterion(logits, smoothed_distribution.clone().detach())\n",
    "\n",
    "        return loss\n",
    "    \n",
    "class NoamScheduler:\n",
    "    \"\"\"\n",
    "    Implements the learning rate scheduler with warmup.\n",
    "\n",
    "    This learning rate scheduler is designed for models such as the Transformer, where the learning rate starts \n",
    "    small, increases linearly during a warmup phase, and then decays proportionally to the inverse square root \n",
    "    of the step number after the warmup phase.\n",
    "\n",
    "    Attributes:\n",
    "        model_dimension (int): The dimensionality of the model (e.g., d_model in the Transformer).\n",
    "        factor (float): A constant scaling factor for the learning rate.\n",
    "        warmup_steps (int): The number of warmup steps before the learning rate begins to decay.\n",
    "        optimizer (Optimizer): The optimizer whose learning rate will be updated.\n",
    "        step_count (int): The current step count in the training process.\n",
    "        current_rate (float): The current learning rate after the last update.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_dimension: int, factor: float, warmup_steps: int, optimizer: Optimizer):\n",
    "        \"\"\"\n",
    "        Initializes the NoamScheduler.\n",
    "\n",
    "        Parameters:\n",
    "            model_dimension (int): The dimensionality of the model (e.g., d_model in Transformer).\n",
    "            factor (float): A scaling factor for the learning rate.\n",
    "            warmup_steps (int): The number of warmup steps before learning rate decay.\n",
    "            optimizer (Optimizer): The optimizer that will be updated with the learning rate.\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        self.step_count = 0\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.factor = factor\n",
    "        self.model_dimension = model_dimension\n",
    "        self.current_rate = 0.0\n",
    "\n",
    "    def step(self) -> None:\n",
    "        \"\"\"\n",
    "        Update the learning rate and step count, then apply the optimizer step.\n",
    "\n",
    "        This method should be called after every batch in training to update the learning rate based on the current step.\n",
    "        \"\"\"\n",
    "        self.step_count += 1\n",
    "        rate = self.compute_rate()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = rate\n",
    "        self.current_rate = rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def compute_rate(self, step: Optional[int] = None) -> float:\n",
    "        \"\"\"\n",
    "        Compute the learning rate based on the current step or a given step.\n",
    "\n",
    "        Parameters:\n",
    "            step (Optional[int]): A specific step to compute the learning rate for. If None, uses the current step count.\n",
    "\n",
    "        Returns:\n",
    "            float: The calculated learning rate based on the Noam formula.\n",
    "        \"\"\"\n",
    "        if step is None:\n",
    "            step = self.step_count\n",
    "\n",
    "        return self.factor * (self.model_dimension ** -0.5) * min(step ** -0.5, step * self.warmup_steps ** -1.5)\n",
    "    \n",
    "class LossComputer:\n",
    "    \"\"\"\n",
    "    A simple loss computation and optimization class for training.\n",
    "\n",
    "    This class wraps the generator, loss function (criterion), and optimizer, and provides a callable\n",
    "    interface to compute the loss, backpropagate, and update the optimizer (if provided).\n",
    "\n",
    "    Attributes:\n",
    "        generator (nn.Module): The model's generator used to produce final predictions.\n",
    "        criterion (nn.Module): The loss function (criterion) to compute the loss, typically cross-entropy or label smoothing.\n",
    "        optimizer (Optional[Optimizer]): The optimizer used for updating model weights.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, generator: nn.Module, criterion: nn.Module, optimizer: Optional[Optimizer] = None):\n",
    "        \"\"\"\n",
    "        Initializes the LossComputer.\n",
    "\n",
    "        Parameters:\n",
    "            generator (nn.Module): The generator network (usually a linear layer followed by softmax or log-softmax).\n",
    "            criterion (nn.Module): The loss function to compute the error between predicted and true labels.\n",
    "            optimizer (Optional[Optimizer]): An optional optimizer for updating the model parameters.\n",
    "        \"\"\"\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def __call__(self, predictions: torch.Tensor, target: torch.Tensor, normalization: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        Compute the loss, backpropagate, and update the optimizer (if provided).\n",
    "\n",
    "        Parameters:\n",
    "            predictions (torch.Tensor): The output predictions from the model, before applying the generator.\n",
    "            target (torch.Tensor): The ground-truth target labels.\n",
    "            normalization (torch.Tensor): A scalar tensor used to normalize the loss, often the number of non-padding tokens.\n",
    "\n",
    "        Returns:\n",
    "            float: The computed loss, scaled by the normalization factor.\n",
    "        \"\"\"\n",
    "        # Pass the predictions through the generator to get final logits\n",
    "        logits = self.generator(predictions)\n",
    "\n",
    "        # Compute the loss using the criterion\n",
    "        loss = self.criterion(\n",
    "            logits.contiguous().view(-1, logits.size(-1)),\n",
    "            target.contiguous().view(-1)\n",
    "        ) / normalization\n",
    "\n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # If an optimizer is provided, update the model's parameters\n",
    "        if self.optimizer is not None:\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.optimizer.zero_grad()\n",
    "\n",
    "        # Return the loss value multiplied by the normalization factor\n",
    "        return loss.item() * normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d95a78c",
   "metadata": {},
   "source": [
    "## 6: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12429c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_opt = torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
    "optimizer = NoamScheduler(model_dimension=256, factor=1, warmup_steps=2000, optimizer=adam_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38186031",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = LabelSmoothing(vocab_size=tgt_vocab,padding_index=0, smoothing=0.1)\n",
    "loss_func = LossComputer(model.generator, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15a821e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, average loss: 5.822\n",
      "Epoch 1, average loss: 3.627\n",
      "Epoch 2, average loss: 2.825\n",
      "Epoch 3, average loss: 2.180\n",
      "Epoch 4, average loss: 1.786\n",
      "Epoch 5, average loss: 1.585\n",
      "Epoch 6, average loss: 1.413\n",
      "Epoch 7, average loss: 1.272\n",
      "Epoch 8, average loss: 1.172\n",
      "Epoch 9, average loss: 1.079\n",
      "Epoch 10, average loss: 1.004\n",
      "Epoch 11, average loss: 0.941\n",
      "Epoch 12, average loss: 0.891\n",
      "Epoch 13, average loss: 0.844\n",
      "Epoch 14, average loss: 0.802\n",
      "Epoch 15, average loss: 0.763\n",
      "Epoch 16, average loss: 0.728\n",
      "Epoch 17, average loss: 0.701\n",
      "Epoch 18, average loss: 0.674\n",
      "Epoch 19, average loss: 0.646\n",
      "Epoch 20, average loss: 0.625\n",
      "Epoch 21, average loss: 0.606\n",
      "Epoch 22, average loss: 0.589\n",
      "Epoch 23, average loss: 0.570\n",
      "Epoch 24, average loss: 0.555\n",
      "Epoch 25, average loss: 0.539\n",
      "Epoch 26, average loss: 0.527\n",
      "Epoch 27, average loss: 0.515\n",
      "Epoch 28, average loss: 0.503\n",
      "Epoch 29, average loss: 0.487\n",
      "Epoch 30, average loss: 0.479\n",
      "Epoch 31, average loss: 0.469\n",
      "Epoch 32, average loss: 0.460\n",
      "Epoch 33, average loss: 0.450\n",
      "Epoch 34, average loss: 0.444\n",
      "Epoch 35, average loss: 0.435\n",
      "Epoch 36, average loss: 0.427\n",
      "Epoch 37, average loss: 0.421\n",
      "Epoch 38, average loss: 0.413\n",
      "Epoch 39, average loss: 0.407\n",
      "Epoch 40, average loss: 0.401\n",
      "Epoch 41, average loss: 0.396\n",
      "Epoch 42, average loss: 0.391\n",
      "Epoch 43, average loss: 0.385\n",
      "Epoch 44, average loss: 0.381\n",
      "Epoch 45, average loss: 0.376\n",
      "Epoch 46, average loss: 0.371\n",
      "Epoch 47, average loss: 0.366\n",
      "Epoch 48, average loss: 0.363\n",
      "Epoch 49, average loss: 0.358\n",
      "Epoch 50, average loss: 0.355\n",
      "Epoch 51, average loss: 0.352\n",
      "Epoch 52, average loss: 0.347\n",
      "Epoch 53, average loss: 0.344\n",
      "Epoch 54, average loss: 0.340\n",
      "Epoch 55, average loss: 0.337\n",
      "Epoch 56, average loss: 0.334\n",
      "Epoch 57, average loss: 0.331\n",
      "Epoch 58, average loss: 0.328\n",
      "Epoch 59, average loss: 0.325\n",
      "Epoch 60, average loss: 0.322\n",
      "Epoch 61, average loss: 0.319\n",
      "Epoch 62, average loss: 0.317\n",
      "Epoch 63, average loss: 0.314\n",
      "Epoch 64, average loss: 0.312\n",
      "Epoch 65, average loss: 0.310\n",
      "Epoch 66, average loss: 0.307\n",
      "Epoch 67, average loss: 0.304\n",
      "Epoch 68, average loss: 0.303\n",
      "Epoch 69, average loss: 0.300\n",
      "Epoch 70, average loss: 0.298\n",
      "Epoch 71, average loss: 0.296\n",
      "Epoch 72, average loss: 0.294\n",
      "Epoch 73, average loss: 0.291\n",
      "Epoch 74, average loss: 0.291\n",
      "Epoch 75, average loss: 0.288\n",
      "Epoch 76, average loss: 0.287\n",
      "Epoch 77, average loss: 0.285\n",
      "Epoch 78, average loss: 0.282\n",
      "Epoch 79, average loss: 0.281\n",
      "Epoch 80, average loss: 0.279\n",
      "Epoch 81, average loss: 0.278\n",
      "Epoch 82, average loss: 0.276\n",
      "Epoch 83, average loss: 0.275\n",
      "Epoch 84, average loss: 0.273\n",
      "Epoch 85, average loss: 0.271\n",
      "Epoch 86, average loss: 0.270\n",
      "Epoch 87, average loss: 0.269\n",
      "Epoch 88, average loss: 0.267\n",
      "Epoch 89, average loss: 0.265\n",
      "Epoch 90, average loss: 0.264\n",
      "Epoch 91, average loss: 0.263\n",
      "Epoch 92, average loss: 0.262\n",
      "Epoch 93, average loss: 0.260\n",
      "Epoch 94, average loss: 0.260\n",
      "Epoch 95, average loss: 0.258\n",
      "Epoch 96, average loss: 0.257\n",
      "Epoch 97, average loss: 0.256\n",
      "Epoch 98, average loss: 0.255\n",
      "Epoch 99, average loss: 0.255\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NEPOCH):\n",
    "    model.train()\n",
    "    tloss=0\n",
    "    tokens=0\n",
    "    for batch in batches:\n",
    "        out = model(batch.src, batch.trg, \n",
    "                    batch.src_mask, batch.trg_mask)\n",
    "        \n",
    "        loss = loss_func(out, batch.trg_y, batch.ntokens)\n",
    "        tloss += loss\n",
    "        tokens += batch.ntokens\n",
    "    print(f\"Epoch {epoch}, average loss: {tloss/tokens:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b3c78ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb6a10b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"models/translation/model_{NEPOCH}.pth\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed31f781",
   "metadata": {},
   "source": [
    "## 7: Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3c740ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_convert_to_indices(sentence: str, tokenizer, vocab_dict: dict, unk_token: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Tokenizes the input sentence and converts tokens to their corresponding indices.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input sentence to tokenize and convert.\n",
    "        tokenizer: The tokenizer used to split the sentence into tokens.\n",
    "        vocab_dict (dict): A dictionary mapping tokens to their respective indices.\n",
    "        unk_token (int): The index to use for unknown tokens.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of token indices on the specified DEVICE.\n",
    "    \"\"\"\n",
    "    tokenized_sentence = [\"BOS\"] + tokenizer.tokenize(sentence) + [\"EOS\"]\n",
    "    token_indices = [vocab_dict.get(token, unk_token) for token in tokenized_sentence]\n",
    "    return torch.tensor(token_indices).long().to(DEVICE).unsqueeze(0)\n",
    "\n",
    "def create_src_mask(src_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Creates a source mask for the input tensor to hide padding tokens.\n",
    "\n",
    "    Args:\n",
    "        src_tensor (torch.Tensor): The source tensor for which to create the mask.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A mask for the source tensor with padding tokens masked out.\n",
    "    \"\"\"\n",
    "    return (src_tensor != 0).unsqueeze(-2)\n",
    "\n",
    "def generate_translation(model, memory: torch.Tensor, src_mask: torch.Tensor, \n",
    "                         start_symbol: int, idx_to_token: dict) -> list:\n",
    "    \"\"\"\n",
    "    Generates a translation for the input sequence in an autoregressive fashion.\n",
    "\n",
    "    Args:\n",
    "        model: The transformer model to use for decoding.\n",
    "        memory (torch.Tensor): The encoded memory from the source sentence.\n",
    "        src_mask (torch.Tensor): The mask for the source sentence.\n",
    "        start_symbol (int): The start symbol in the target vocabulary.\n",
    "        idx_to_token (dict): A dictionary mapping indices to tokens in the target language.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of translated tokens.\n",
    "    \"\"\"\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).long().to(DEVICE)  \n",
    "    translation_tokens = []\n",
    "\n",
    "    for _ in range(MAX_LENGTH):\n",
    "        out = model.decode(memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(ys))\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        if idx_to_token[next_word] == 'EOS':\n",
    "            break\n",
    "\n",
    "        ys = torch.cat([ys, torch.ones(1, 1).fill_(next_word).long().to(DEVICE)], dim=1)  \n",
    "        translation_tokens.append(idx_to_token[next_word])\n",
    "\n",
    "    return translation_tokens\n",
    "\n",
    "def post_process_translation(tokens: list) -> str:\n",
    "    \"\"\"\n",
    "    Post-processes the list of translated tokens into a coherent sentence.\n",
    "\n",
    "    Args:\n",
    "        tokens (list): List of tokens representing the translation.\n",
    "\n",
    "    Returns:\n",
    "        str: The final post-processed sentence.\n",
    "    \"\"\"\n",
    "    sentence = \"\".join(tokens)\n",
    "    sentence = sentence.replace(\"</w>\", \" \")\n",
    "\n",
    "    punctuation = '''?:;.,'(\"-!&)%'''\n",
    "    for punc in punctuation:\n",
    "        sentence = sentence.replace(f\" {punc}\", f\"{punc}\")\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def translate(\n",
    "    sentence: str, \n",
    "    model: nn.Module, \n",
    "    tokenizer, \n",
    "    source_vocab: dict, \n",
    "    target_vocab: dict, \n",
    "    idx_to_target_token: dict\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Translates a given English sentence into the target language using a transformer model.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The English sentence to translate.\n",
    "        model (nn.Module): The transformer model used for translation.\n",
    "        tokenizer: Tokenizer to convert the sentence into tokens.\n",
    "        source_vocab (dict): Dictionary mapping English tokens to their corresponding indices.\n",
    "        target_vocab (dict): Dictionary mapping target language tokens to their corresponding indices.\n",
    "        idx_to_target_token (dict): Dictionary mapping indices back to the target language tokens.\n",
    "\n",
    "    Returns:\n",
    "        str: The translated sentence in the target language.\n",
    "    \"\"\"\n",
    "    # Step 1: Tokenize the English sentence and convert tokens to indices\n",
    "    source_tensor = tokenize_and_convert_to_indices(sentence, tokenizer, source_vocab, UNK)\n",
    "    \n",
    "    # Step 2: Create a source mask to mask out padding tokens\n",
    "    source_mask = create_src_mask(source_tensor)\n",
    "    \n",
    "    # Step 3: Encode the input sentence into memory using the transformer model\n",
    "    memory = model.encode(source_tensor, source_mask)\n",
    "    \n",
    "    # Step 4: Autoregressively generate the translated sentence in the target language\n",
    "    start_symbol = target_vocab[\"BOS\"]\n",
    "    translation_tokens = generate_translation(\n",
    "        model, memory, source_mask, \n",
    "        start_symbol=start_symbol, idx_to_token=idx_to_target_token\n",
    "    )\n",
    "    \n",
    "    # Step 5: Post-process the translation tokens into a readable sentence\n",
    "    translated_sentence = post_process_translation(translation_tokens)\n",
    "    \n",
    "    return translated_sentence.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "208a1a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "trained_weights=torch.load(f\"models/translation/model_{NEPOCH}.pth\", map_location=DEVICE)\n",
    "model.load_state_dict(trained_weights)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "331b50f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = 'he is happy.'\n",
    "# sentence = 'he is not happy and he wants to go to the police.'\n",
    "sentence = 'my wife wants to eat chicken.'\n",
    "# sentence = \"I love skiing in the winter!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cf8bbd59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ma femme veut manger du poulet.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(sentence, \n",
    "          model, \n",
    "          tokenizer, \n",
    "          en_word_dict, \n",
    "          fr_word_dict, \n",
    "          fr_idx_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc0d104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
