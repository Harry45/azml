{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c795b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import Counter \n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e6328cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE =\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "NEPOCHS = 20\n",
    "EVERY = 5\n",
    "LR = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e6e0af",
   "metadata": {},
   "source": [
    "## 1 : Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbd6c9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    \"\"\"\n",
    "    Process text from multiple files, replacing specific characters and combining the content.\n",
    "\n",
    "    This function reads three text files, applies character replacements to the first file\n",
    "    (modifying quotes and apostrophes), and concatenates the contents of all three files into a single string.\n",
    "\n",
    "    The transformations include:\n",
    "    - Replacing double quotes with curly quotes based on context.\n",
    "    - Replacing single quotes with curly apostrophes based on surrounding characters.\n",
    "\n",
    "    Returns:\n",
    "        str: The combined and processed text from all three files.\n",
    "    \"\"\"\n",
    "    # Read the first text file and process the quotes and apostrophes\n",
    "    with open(\"data/generation/OldManAndSea.txt\", \"r\", encoding='utf-8-sig') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    text = list(text)  # Convert text to a list for easier character manipulation\n",
    "    for i in range(len(text)):\n",
    "        if text[i] == '\"':\n",
    "            # If followed by a space or newline, replace with closing curly quote\n",
    "            if text[i + 1] == ' ' or text[i + 1] == '\\n':\n",
    "                text[i] = '”'\n",
    "            # If not followed by a space or newline, replace with opening curly quote\n",
    "            else:\n",
    "                text[i] = '“'\n",
    "        if text[i] == \"'\":\n",
    "            # If preceded by a non-space or non-newline character, replace with closing curly apostrophe\n",
    "            if text[i - 1] != ' ' and text[i - 1] != '\\n':\n",
    "                text[i] = '’'\n",
    "    \n",
    "    text = \"\".join(text)  # Convert the list back into a string\n",
    "\n",
    "    # Read the second and third text files\n",
    "    with open(\"data/generation/ToWhomTheBellTolls.txt\", \"r\", encoding='utf-8-sig') as f:\n",
    "        text1 = f.read()\n",
    "\n",
    "    with open(\"data/generation/FarewellToArms.txt\", \"r\", encoding='utf-8-sig') as f:\n",
    "        text2 = f.read()\n",
    "\n",
    "    # Combine the processed text from all three files\n",
    "    combined_text = f\"{text} {text1} {text2}\"\n",
    "    \n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ed25fe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(text):\n",
    "    \"\"\"\n",
    "    Build a word-to-integer and integer-to-word vocabulary from the input text.\n",
    "\n",
    "    This function processes the input text by:\n",
    "    - Converting it to lowercase.\n",
    "    - Adding spaces around punctuation.\n",
    "    - Tokenizing the text into words.\n",
    "    - Counting word frequencies.\n",
    "    - Creating a mapping of words to integers and vice versa.\n",
    "\n",
    "    An 'UNK' token is added to represent unknown words.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be processed.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists and two dictionaries:\n",
    "            - punctuations (list): a list of all the punctuations.\n",
    "            - tokens (list): a list of all the tokens.\n",
    "            - word_to_int (dict): Mapping from words to their corresponding integer indices.\n",
    "            - int_to_word (dict): Mapping from integer indices to their corresponding words.\n",
    "    \"\"\"\n",
    "    # Convert text to lowercase and replace newlines with spaces\n",
    "    text = text.lower().replace(\"\\n\", \" \")\n",
    "    \n",
    "    # Identify all unique characters in the text and extract punctuations\n",
    "    unique_chars = set(text)\n",
    "    punctuations = [char for char in unique_chars if not char.isalnum()]\n",
    "    \n",
    "    # Add spaces around each punctuation in the text\n",
    "    for punctuation in punctuations:\n",
    "        text = text.replace(punctuation, f\" {punctuation} \")\n",
    "\n",
    "    # Tokenize the text into words (splitting by spaces)\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Count occurrences of each word in the tokenized text\n",
    "    word_frequencies = Counter(tokens)\n",
    "\n",
    "    # Sort words by their frequency in descending order\n",
    "    sorted_words = sorted(word_frequencies, key=word_frequencies.get, reverse=True)\n",
    "\n",
    "    # Add the 'UNK' (unknown) token to handle unseen words\n",
    "    sorted_words.append(\"UNK\")\n",
    "\n",
    "    # Create a mapping from words to integers and from integers to words\n",
    "    word_to_int = {word: idx for idx, word in enumerate(sorted_words)}\n",
    "    int_to_word = {idx: word for word, idx in word_to_int.items()}\n",
    "\n",
    "    return punctuations, tokens, word_to_int, int_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "672e7595",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = process_data()\n",
    "punctuations, tokens, word_to_int, int_to_word = build_vocab(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde7e17a",
   "metadata": {},
   "source": [
    "## 2 : Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4aa1cb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(tokens, word_to_int, seq_len=128, batch_size=32):\n",
    "    \"\"\"\n",
    "    Create batches of input-output pairs for training a language model.\n",
    "\n",
    "    This function converts a list of tokens into their corresponding integer representations\n",
    "    using the `word_to_int` mapping, and then constructs input-output pairs where:\n",
    "    - The input (`x`) is a sequence of `seq_len` consecutive words.\n",
    "    - The output (`y`) is the same sequence, shifted by one word.\n",
    "\n",
    "    These pairs are batched into tensors and returned as a DataLoader for easier training.\n",
    "\n",
    "    Args:\n",
    "        tokens (list of str): A list of word tokens.\n",
    "        word_to_int (dict): A dictionary mapping words to their corresponding integer indices.\n",
    "        seq_len (int, optional): The length of each input sequence. Defaults to 128.\n",
    "        batch_size (int, optional): The number of sequences per batch. Defaults to 32.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: A DataLoader object containing batches of input-output tensor pairs.\n",
    "    \"\"\"\n",
    "    # Convert the tokens into their corresponding integer indices\n",
    "    token_indices = [word_to_int[word] for word in tokens]\n",
    "\n",
    "    # Prepare input-output pairs for each sequence\n",
    "    input_output_pairs = []\n",
    "    for i in range(0, len(token_indices) - seq_len):\n",
    "        input_seq = token_indices[i:i + seq_len]\n",
    "        target_seq = token_indices[i + 1:i + seq_len + 1]\n",
    "        input_output_pairs.append((torch.tensor(input_seq), torch.tensor(target_seq)))\n",
    "\n",
    "    # Create a DataLoader with the given batch size, shuffling the data\n",
    "    data_loader = DataLoader(input_output_pairs, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38f5220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = create_batches(tokens, word_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1d65da",
   "metadata": {},
   "source": [
    "## 3 : GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0c597b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"\n",
    "    Configuration class for a Transformer-based language model.\n",
    "\n",
    "    This class defines the architecture and hyperparameters for the model,\n",
    "    such as the number of layers, heads, embedding size, vocabulary size, \n",
    "    sequence length, and dropout rates. These parameters can be used to \n",
    "    initialize and fine-tune the model.\n",
    "\n",
    "    Attributes:\n",
    "        vocab_size (int): Size of the vocabulary (number of unique tokens).\n",
    "        num_layers (int): Number of layers in the transformer model.\n",
    "        num_heads (int): Number of attention heads per layer.\n",
    "        embedding_dim (int): Dimensionality of the embedding vectors.\n",
    "        max_sequence_length (int): Maximum length of the input sequences.\n",
    "        embedding_dropout (float): Dropout rate for the embedding layer.\n",
    "        residual_dropout (float): Dropout rate for residual connections.\n",
    "        attention_dropout (float): Dropout rate for attention weights.\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_size: int\n",
    "    num_layers: int = 3\n",
    "    num_heads: int = 4\n",
    "    embedding_dim: int = 256\n",
    "    max_sequence_length: int = 128\n",
    "    embedding_dropout: float = 0.1\n",
    "    residual_dropout: float = 0.1\n",
    "    attention_dropout: float = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "99876a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_int)\n",
    "config = ModelConfig(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e53824",
   "metadata": {},
   "source": [
    "#### 3.1 : Causal Self-Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eaa0cf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements causal self-attention for transformer models.\n",
    "\n",
    "    This class defines the operations for self-attention, which includes:\n",
    "    - Linear projections to obtain query, key, and value vectors.\n",
    "    - Scaled dot-product attention with a causal mask to ensure that future tokens\n",
    "      in the sequence do not influence predictions of the current token.\n",
    "    - Dropout layers applied to the attention and output projections.\n",
    "\n",
    "    Attributes:\n",
    "        query_key_value_proj (nn.Linear): Linear layer to compute query, key, and value vectors.\n",
    "        output_proj (nn.Linear): Linear layer to project the output back to the embedding dimension.\n",
    "        attention_dropout (nn.Dropout): Dropout applied to attention weights.\n",
    "        residual_dropout (nn.Dropout): Dropout applied to the residual connection.\n",
    "        bias_mask (torch.Tensor): Causal mask to prevent attending to future tokens.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        embedding_dim (int): Dimensionality of the input embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initializes the CausalSelfAttention module with the specified configuration.\n",
    "\n",
    "        Args:\n",
    "            config (ModelConfig): Configuration object containing model parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.query_key_value_proj = nn.Linear(config.embedding_dim, 3 * config.embedding_dim)\n",
    "        self.output_proj = nn.Linear(config.embedding_dim, config.embedding_dim)\n",
    "        self.attention_dropout = nn.Dropout(config.attention_dropout)\n",
    "        self.residual_dropout = nn.Dropout(config.residual_dropout)\n",
    "\n",
    "        # Create a causal mask for self-attention (upper triangular mask to prevent attending to future tokens)\n",
    "        self.register_buffer(\n",
    "            \"bias_mask\", \n",
    "            torch.tril(torch.ones(config.max_sequence_length, config.max_sequence_length))\n",
    "                 .view(1, 1, config.max_sequence_length, config.max_sequence_length)\n",
    "        )\n",
    "\n",
    "        self.num_heads = config.num_heads\n",
    "        self.embedding_dim = config.embedding_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the causal self-attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length, embedding_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of the same shape as the input.\n",
    "        \"\"\"\n",
    "        batch_size, sequence_length, embedding_dim = x.size()\n",
    "\n",
    "        # Compute query, key, and value vectors from input\n",
    "        query, key, value = self.query_key_value_proj(x).split(self.embedding_dim, dim=2)\n",
    "\n",
    "        # Reshape query, key, and value for multi-head attention\n",
    "        head_size = embedding_dim // self.num_heads\n",
    "        query = query.view(batch_size, sequence_length, self.num_heads, head_size).transpose(1, 2)\n",
    "        key = key.view(batch_size, sequence_length, self.num_heads, head_size).transpose(1, 2)\n",
    "        value = value.view(batch_size, sequence_length, self.num_heads, head_size).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(head_size)\n",
    "        \n",
    "        # Apply causal mask to prevent attention to future tokens\n",
    "        attention_scores = attention_scores.masked_fill(self.bias_mask[:, :, :sequence_length, :sequence_length] == 0, float('-inf'))\n",
    "        \n",
    "        # Compute attention weights\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        attention_weights = self.attention_dropout(attention_weights)\n",
    "\n",
    "        # Compute the weighted sum of value vectors\n",
    "        attention_output = attention_weights @ value\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, sequence_length, embedding_dim)\n",
    "\n",
    "        # Apply output projection and residual dropout\n",
    "        output = self.residual_dropout(self.output_proj(attention_output))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad214191",
   "metadata": {},
   "source": [
    "#### 3.2 GPT Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "70f349ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single transformer block consisting of layer normalization, causal self-attention,\n",
    "    and a feed-forward multi-layer perceptron (MLP).\n",
    "\n",
    "    This block forms the core of a transformer architecture, combining self-attention \n",
    "    and a feed-forward network with residual connections and normalization.\n",
    "\n",
    "    Attributes:\n",
    "        layer_norm_1 (nn.LayerNorm): First layer normalization before the attention layer.\n",
    "        attention (CausalSelfAttention): Causal self-attention layer.\n",
    "        layer_norm_2 (nn.LayerNorm): Second layer normalization before the MLP.\n",
    "        mlp (nn.Sequential): Feed-forward network with GELU activation and dropout.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initializes the transformer block with the specified configuration.\n",
    "\n",
    "        Args:\n",
    "            config (ModelConfig): Configuration object containing model parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # First layer normalization before self-attention\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.embedding_dim)\n",
    "        \n",
    "        # Causal self-attention module\n",
    "        self.attention = CausalSelfAttention(config)\n",
    "        \n",
    "        # Second layer normalization before the MLP\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.embedding_dim)\n",
    "        \n",
    "        # Feed-forward network (MLP) with activation and dropout\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.embedding_dim, 4 * config.embedding_dim), \n",
    "            nn.GELU(),  \n",
    "            nn.Linear(4 * config.embedding_dim, config.embedding_dim),\n",
    "            nn.Dropout(config.residual_dropout)  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer block.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length, embedding_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of the same shape as the input.\n",
    "        \"\"\"\n",
    "        # Apply layer normalization and self-attention with residual connection\n",
    "        x = x + self.attention(self.layer_norm_1(x))\n",
    "        \n",
    "        # Apply layer normalization and MLP with residual connection\n",
    "        x = x + self.mlp(self.layer_norm_2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aeaeca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A transformer-based language model for autoregressive text generation.\n",
    "\n",
    "    This model is composed of multiple transformer blocks, each consisting of self-attention\n",
    "    and feed-forward networks. Positional embeddings are added to token embeddings to retain\n",
    "    the order of the input sequence. The model can be used for next-token prediction.\n",
    "\n",
    "    Attributes:\n",
    "        max_sequence_length (int): Maximum length of the input sequences.\n",
    "        transformer (nn.ModuleDict): Dictionary containing the embedding layers, transformer blocks, \n",
    "                                     and the final layer normalization.\n",
    "        lm_head (nn.Linear): Linear layer for mapping transformer outputs to the vocabulary size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initializes the transformer language model with the specified configuration.\n",
    "\n",
    "        Args:\n",
    "            config (ModelConfig): Configuration object containing model parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_sequence_length = config.max_sequence_length\n",
    "        \n",
    "        # Transformer components\n",
    "        self.transformer = nn.ModuleDict({\n",
    "            \"token_embedding\": nn.Embedding(config.vocab_size, config.embedding_dim),  \n",
    "            \"position_embedding\": nn.Embedding(config.max_sequence_length, config.embedding_dim),  \n",
    "            \"dropout\": nn.Dropout(config.embedding_dropout), \n",
    "            \"transformer_blocks\": nn.ModuleList([TransformerBlock(config) \n",
    "                                                 for _ in range(config.num_layers)]),\n",
    "            \"layer_norm\": nn.LayerNorm(config.embedding_dim)  \n",
    "        })\n",
    "        \n",
    "        # Output head: project the transformer output back to the vocabulary size for token prediction\n",
    "        self.lm_head = nn.Linear(config.embedding_dim, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Custom initialization for projection layers in the MLP of each block\n",
    "        self._initialize_parameters(config)\n",
    "\n",
    "    def _initialize_parameters(self, config):\n",
    "        \"\"\"\n",
    "        Custom initialization for the projection weights in the model.\n",
    "\n",
    "        Args:\n",
    "            config (ModelConfig): Configuration object containing model parameters.\n",
    "        \"\"\"\n",
    "        for param_name, param in self.named_parameters():\n",
    "            if param_name.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(param, mean=0.0, std=0.02 / math.sqrt(2 * config.num_layers))\n",
    "\n",
    "    def forward(self, input_indices, targets=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the transformer model.\n",
    "\n",
    "        Args:\n",
    "            input_indices (torch.Tensor): Input tensor of shape (batch_size, sequence_length) containing token indices.\n",
    "            targets (torch.Tensor, optional): Target tensor for calculating loss (if any). Default is None.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Logits of shape (batch_size, sequence_length, vocab_size) representing the next-token predictions.\n",
    "        \"\"\"\n",
    "        batch_size, sequence_length = input_indices.size()\n",
    "\n",
    "        # Generate position indices for positional embeddings\n",
    "        position_indices = torch.arange(0, sequence_length, dtype=torch.long).unsqueeze(0).to(input_indices.device)\n",
    "\n",
    "        # Token and position embeddings\n",
    "        token_embeddings = self.transformer[\"token_embedding\"](input_indices)\n",
    "        position_embeddings = self.transformer[\"position_embedding\"](position_indices)\n",
    "\n",
    "        # Combine token and positional embeddings, then apply dropout\n",
    "        x = self.transformer[\"dropout\"](token_embeddings + position_embeddings)\n",
    "\n",
    "        # Pass through each transformer block\n",
    "        for block in self.transformer[\"transformer_blocks\"]:\n",
    "            x = block(x)\n",
    "\n",
    "        # Final layer normalization\n",
    "        x = self.transformer[\"layer_norm\"](x)\n",
    "\n",
    "        # Output logits for vocabulary prediction\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2360fc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 5.12 M\n"
     ]
    }
   ],
   "source": [
    "model=TransformerLanguageModel(config)\n",
    "model.to(DEVICE)\n",
    "num=sum(p.numel() for p in model.transformer.parameters())\n",
    "print(f\"Number of parameters: {num/1e6:.2f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb768e64",
   "metadata": {},
   "source": [
    "## 4 : Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "30c0a337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, num_epochs, learning_rate, save_frequency):\n",
    "    \"\"\"\n",
    "    Train a given model using the specified data loader.\n",
    "\n",
    "    This function performs the training loop for a specified number of epochs,\n",
    "    calculating the loss and updating the model parameters using the Adam optimizer.\n",
    "    The model's state is saved at specified intervals.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to be trained.\n",
    "        data_loader (DataLoader): DataLoader providing batches of training data.\n",
    "        num_epochs (int): The number of epochs to train the model.\n",
    "        learning_rate (float): The learning rate for the optimizer.\n",
    "        save_frequency (int): Frequency of saving the model state (in epochs).\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    model.train()  \n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        total_loss = 0.0\n",
    "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_function(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Average loss for the epoch\n",
    "        average_loss = total_loss / (batch_idx + 1)\n",
    "        print(f'Epoch {epoch} loss: {average_loss:.3f}')\n",
    "\n",
    "        # Save model state if necessary\n",
    "        if epoch % save_frequency == 0:\n",
    "            torch.save(model.state_dict(), f'models/generation/GPT_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8ed5a523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 2.129\n",
      "Epoch 2 loss: 1.770\n",
      "Epoch 3 loss: 1.512\n",
      "Epoch 4 loss: 1.316\n",
      "Epoch 5 loss: 1.164\n",
      "Epoch 6 loss: 1.044\n",
      "Epoch 7 loss: 0.945\n",
      "Epoch 8 loss: 0.865\n",
      "Epoch 9 loss: 0.800\n",
      "Epoch 10 loss: 0.746\n",
      "Epoch 11 loss: 0.700\n",
      "Epoch 12 loss: 0.662\n",
      "Epoch 13 loss: 0.629\n",
      "Epoch 14 loss: 0.600\n",
      "Epoch 15 loss: 0.575\n",
      "Epoch 16 loss: 0.553\n",
      "Epoch 17 loss: 0.533\n",
      "Epoch 18 loss: 0.516\n",
      "Epoch 19 loss: 0.500\n",
      "Epoch 20 loss: 0.485\n"
     ]
    }
   ],
   "source": [
    "train_model(model, loader, NEPOCHS, LR, EVERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62028f6c",
   "metadata": {},
   "source": [
    "## 5 : Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5179e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(config, input_indices, model, max_new_tokens, \n",
    "                     temperature=1.0, top_k=None):\n",
    "    \"\"\"\n",
    "    Generate new token samples from the language model based on a given input sequence.\n",
    "\n",
    "    This function iteratively predicts the next token(s) using the model and appends them\n",
    "    to the input sequence. It allows for the adjustment of the sampling strategy through\n",
    "    temperature scaling and top-k filtering.\n",
    "\n",
    "    Args:\n",
    "        config (ModelConfig): Configuration object containing model parameters.\n",
    "        input_indices (torch.Tensor): Input tensor of shape (batch_size, sequence_length) containing token indices.\n",
    "        model (nn.Module): The language model used for generating samples.\n",
    "        max_new_tokens (int): Maximum number of new tokens to generate.\n",
    "        temperature (float, optional): Controls the randomness of predictions. Default is 1.0.\n",
    "        top_k (int, optional): If specified, only the top k tokens will be considered for sampling. Default is None.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (batch_size, new_sequence_length) containing the newly generated tokens.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    original_length = input_indices.size(1)  # Keep track of the original length of the input indices\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Trim the input if it exceeds the maximum allowed length\n",
    "        if input_indices.size(1) <= config.max_sequence_length:\n",
    "            conditional_indices = input_indices\n",
    "        else:\n",
    "            conditional_indices = input_indices[:, -config.max_sequence_length:]\n",
    "\n",
    "        # Predict the logits for the next token\n",
    "        logits = model(conditional_indices.to(DEVICE))\n",
    "\n",
    "        # Get logits for the last token in the sequence and apply temperature scaling\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "\n",
    "        # Apply top-k filtering if specified\n",
    "        if top_k is not None:\n",
    "            top_values, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < top_values[:, [-1]]] = -float('Inf')\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "\n",
    "        # Sample the next token from the distribution\n",
    "        next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "        \n",
    "        # Append the sampled token to the input indices\n",
    "        input_indices = torch.cat((input_indices, next_token.cpu()), dim=1)\n",
    "\n",
    "    # Return only the newly generated tokens\n",
    "    return input_indices[:, original_length:]\n",
    "\n",
    "def generate_text(config, prompt, model, max_new_tokens, punctuation_marks, word_to_int, \n",
    "                  temperature=1.0, top_k=None):\n",
    "    \"\"\"\n",
    "    Generate text based on a given prompt using a language model.\n",
    "\n",
    "    This function takes an initial prompt, tokenizes it, and generates a specified number of new tokens\n",
    "    using the model. The generated tokens are then formatted and returned as a complete text string.\n",
    "\n",
    "    Args:\n",
    "        config (ModelConfig): Configuration object containing model parameters.\n",
    "        prompt (str): The initial text prompt to generate text from.\n",
    "        model (nn.Module): The language model used for generating new tokens.\n",
    "        max_new_tokens (int): Maximum number of new tokens to generate.\n",
    "        punctuation_marks (list): List of punctuation marks to format in the output text.\n",
    "        word_to_int (dict): Mapping from words to their corresponding integer indices.\n",
    "        temperature (float, optional): Controls the randomness of predictions. Default is 1.0.\n",
    "        top_k (int, optional): If specified, only the top k tokens will be considered for sampling. Default is None.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text, combining the prompt and the newly generated tokens.\n",
    "    \"\"\"\n",
    "    assert len(prompt) > 0, \"Prompt must contain at least one token\"\n",
    "\n",
    "    # Preprocess the prompt: lowercase and replace newlines\n",
    "    processed_text = prompt.lower().replace(\"\\n\", \" \")\n",
    "    \n",
    "    # Add spaces around punctuation marks for tokenization\n",
    "    for punctuation in punctuation_marks:\n",
    "        processed_text = processed_text.replace(f\"{punctuation}\", f\" {punctuation} \")\n",
    "    \n",
    "    # Tokenize the processed text\n",
    "    tokenized_text = processed_text.split() \n",
    "    token_indices = [word_to_int.get(word, word_to_int[\"UNK\"]) for word in tokenized_text]\n",
    "    token_tensor = torch.LongTensor(token_indices).unsqueeze(0)\n",
    "\n",
    "    # Generate new tokens based on the prompt\n",
    "    generated_indices = generate_samples(config, token_tensor, model, max_new_tokens, temperature, top_k)\n",
    "    \n",
    "    # Convert generated indices back to text\n",
    "    generated_tokens = [int_to_word[i] for i in generated_indices.squeeze().numpy()]\n",
    "    generated_text = \" \".join(generated_tokens)\n",
    "\n",
    "    # Format the generated text to remove unwanted spaces around punctuation\n",
    "    for punctuation in '''”).:;!?,-‘’''':\n",
    "        generated_text = generated_text.replace(f\" {punctuation}\", f\"{punctuation}\")\n",
    "    for punctuation in '''“(-‘’''':\n",
    "        generated_text = generated_text.replace(f\"{punctuation} \", f\"{punctuation}\")\n",
    "    \n",
    "    return prompt + \" \" + generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c1a9a496",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "trained_weights=torch.load(f\"models/generation/GPT_20.pth\", map_location=DEVICE)\n",
    "model.load_state_dict(trained_weights)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1aaae1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', over his head. “he’s been around a long line.” it showed bright in the water that ran across his shoulders rose slowly over the hole behind his shoulders. he was touching its tail-gate of the water splashing down from the sudden eruption of a geyser. the deaf man shook his head at robert jordan and grinned in delight. he continued to shake his head happily as pilar went on vilifying and robert jordan knew that it was all right again now. finally she stopped cursing, reached for the water jug, tipped it up and took a drink and said, calmly, “then just shut up about what we are to do afterwards, will you, inglés? you go back to the republic and you take your piece with you and leave us others alone here to decide what part of these hills we’ll die in.” “live in,” el sordo said. he had put his hand in the deep voice up to the table. “live thee,” pilar said. “'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt=\"UNK\"\n",
    "generate_text(config, prompt, model, max_new_tokens=200, \n",
    "              punctuation_marks=punctuations, word_to_int=word_to_int)[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9047da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
